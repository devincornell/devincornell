
---
title: "Patterns for Data Science: Project management"
subtitle: "Some design patterns for improving the way you manage your data science projects."
date: "May 28, 2023"
id: "zods2_project_management"
---






# 2. Version almost everything

My next recommendation is related to managing projects. A key challenge in project management is keeping track of different intermediary data products and the code used to generate them. It is typical to be in a scenario where, halfway through building a data pipeline, your requirements change or you produce results that encourage you to take a different approach to analysis. In that case, you don't want to delete the old intermediary results because you may want to refer back to them later. Or, worse yet, you shared them with a client and may need to refer back to them for their sake. Either way, you also want to keep track of the code you used to create those files, in case you get any questions about it or are simply curious. It is in this scenario that I recommend that you retain multiple class definitions for the same type of object and keep track using naming conventions. Then, when you go to look closer at a result you generated previously, you can easily track the code you used to generate it. 

It might be worth thinking more about typical data science workflows to get a sense of why this might make sense. First, lets take a look at the following figure.

![data science pipeline overview](https://storage.googleapis.com/public_data_09324832787/ds_pipeline_workflow.svg)

At the top of this figure we see the original dataset, composed of one csv file and one json file. Each of the scripts that appear here involve some type of transformation from one data type to another. Script 1 takes the original csv data and converts it into a new Intermediate csv file. Script 3 takes the orignal csv file, the original json file, and the intermediate csv file generated by Script 1, and converts it into Final table csv file. Script 4 then converts this table into a visualization that will be presented to the client. Note that on the right we also see that Script 2 converts the original json data to Some figure png file - possibly for another analysis.

In an ideal world, we work on and finish Script 1, then work on and finish Script 2, and so on until we finish coding and can produce the final data from the original data. In a more practical scenario, most likely will need to go back and fix some upstream script to add or edit some information according to requirement changes or just changing realization of the information needed to produce intermediary results. When we do this, however, it doesn't always make sense to delete data that was previously generated - either so we can use it as reference or present it as intermediary results to the client. In these cases, it makes the most sense to keep track of the data and the code that was used to generate it through an informal versioning system of your design.

For an object-oriented example, let's assume that the client gives you data in the format represented by the class `IrisA`.

    @dataclasses.dataclass
    class IrisA:
        sepal_length: int
        sepal_width: int
        species: str

You write a script that converts `IrisA` data to `IrisB` data using its class method constructor `from_a`, and you saved the result as `resultB.csv`. This result will be used later on by another script to produce the final figure. Lets say you produce the final figure and call it `finalB.png`, then share it with your client as intermediate results. See my example class below.

    @dataclasses.dataclass
    class IrisB:
        sepal_area: float
        
        @classmethod
        def from_a(cls, a: IrisA):
            return cls(sepal_area = a.sepal_length * a.sepal_width)

The client has some suggested improvements, and you realize you need one additional piece of information from the `IrisA` dataset to be included in `IrisB`: the `species`. So you go back and edit `IrisB` to include this information. You use this dataset to produce the final result `finalB2.png`, and share with the client. 

    @dataclasses.dataclass
    class IrisB:
        sepal_area: float
        species: str
        
        @classmethod
        def from_a(cls, a: IrisA):
            return cls(sepal_area = a.sepal_length * a.sepal_width, species=a.species)


The client asks about a discrepency between `finalB.png` and `finalB2.png` - can you answer their question? You could go back to the correct commit based on datetimes, but that might be a little tricky if the file timestamp was changed after copying/etc. Instead, I recommend creating a versioning system that is consistent between the names of your data objects and the intermediate data files they were used to generate.

    @dataclasses.dataclass
    class IrisB_v1:
        sepal_area: float
        
        @classmethod
        def from_a(cls, a: IrisA):
            return cls(sepal_area = a.sepal_length * a.sepal_width)

    @dataclasses.dataclass
    class IrisB_v2:
        sepal_area: float
        species: str
        
        @classmethod
        def from_a(cls, a: IrisA):
            return cls(sepal_area = a.sepal_length * a.sepal_width, species=a.species)

When you save the files, you could integrate the name of the class into the filename. For instance, you could call them `result_IrisB_v1.csv` and `result_IrisB_v2.csv`, which you could then translate to a final figure names like `final_result_IrisB_v1.png` `final_result_IrisB_v2.png`. This is what it might look like:

    a = IrisA(1.0, 2.0, 'big')
    
    b1 = IrisB_v1.from_a(a)
    save(b1, f'result_{b1.__class__}.csv')

    b2 = IrisB_v2.from_a(a)
    save(b2, f'result_{b2.__class__}.csv')

Of course every case is unique and I don't recommend using this everywhere, but I have found it to be useful in scenarios where I am getting a lot of client feedback and I need to refer to old code when I'm trying to explain differences between results figures or intermediate data.

# 3. Create parameter or settings objects that live in your code

Building off of the previous two examples, I recommend creating parameter objects that you can use to keep track of results created with different parameters. This essentially follows from the previous two examples, but further introduces the possibility that you may generate results from many different parameter sets over the lifetime of your project. Ideally the defined parameter set will be used at every step of your data pipeline to make it perfectly reproducable. Lets see the example below.

Start by creating a dataclass to contain parameter information. This defines all the variables that will be used to generate results throughout your data pipeline. It might be helpful to have some `version_name` member to make it easy to get the version as a string. This even includes a parameter that accepts a type - this will allow you to control which version of a class is being used throughout your pipeline. This may be overkill, but you can imagine projects that are of sufficient complexity as to benefit to this level of detail.

    @dataclasses.dataclass
    class Params:
        version_name: str
        IrisB: type
        intermediate_fname: str
        final_fname: str

Now I recommend creating functions or module-level variables for param object instances, including the version number as part of the object name. Here I have two params objecs, which I named `0x1` and `0x2`.

    def params_0x1() -> Params:
        return Params(
            version_name = '0x1',
            IrisB = IrisB_v1,
            intermediate_fname = 'intermediate_0x1.csv',
            final_fname = 'final_0x1.csv',
        )
        
    def params_0x2() -> Params:
        return Params(
            version_name = '0x2',
            IrisB = IrisB_v2,
            intermediate_fname = 'intermediate_0x2.csv',
            final_fname = 'final_0x2.csv',
        )

So at the beginning of each script you can instantiate a params object and use it at various points.

    params = params_0x1()
    a = IrisA(1.0, 2.0, 'big')
    b = params.IrisB.from_a(a)
    save(b, f'result_{params.version_name}.csv')

This approach may be overkill in some scenarios, but if the project gets large enough it is an option you may want to consider.


# 4. Make I/O explicit in your top-level scripts

This principle is very simple: make it such the user can see save and read functions at the top level of your script (even if the filenames are hidden). It should be clear to the reader that the script is ingesting one or more datasets and exporting others. Adding this to your script can save you a lot of time later when you try to determine which types of data this script makes and outputs.

    df = pd.read_csv('test.csv')
    new_data = myfunc(df)
    new_data.save_json('new_data.json')

This can still be consistent with previous recommendations as long as you access filename properties at the top level of your script. In this example, the filenames are stored as members of the `params` object.

    params = params_0x1()
    df = pd.read_csv(params.test_csv_fname)
    new_data = myfunc(df)
    new_data.save_json(params.output_json_fname)
