<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Devin J. Cornell: Generating Visual Art Using Deep Learning Models</title>
        <link rel="icon" type="image/x-icon" href="/assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <!--<script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js" crossorigin="anonymous"></script>-->
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="/css/blog.css" rel="stylesheet" />
    </head>
    <body>
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
            <div class="container px-4 px-lg-5">
                <a class="navbar-brand" href="/">devinjcornell.com</a>//<a class="navbar-brand" href="/blog">Blog</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto py-4 py-lg-0">
                        <!--<li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="index.html">Home</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="about.html">About</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="post.html">Sample Post</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="contact.html">Contact</a></li>-->
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Page Header-->
        <header class="masthead" style="background-image: url('assets/img/post-bg.jpg')">
            <div class="container position-relative px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <div class="post-heading">
                            <h1>Generating Visual Art Using Deep Learning Models</h1>
                            <h2 class="subheading">Some examples of techniques and approaches I've been using to generate visual art using deep learning models.</h2>
                            <span class="meta">
                                Posted by
                                <a href="/">Devin J. Cornell</a>
                                on February 6, 2022
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <!-- Post Content-->
        <article class="mb-4">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <p>I've recently been playing with approaches to generate visual art using machine learning models. Inspired by <a href="https://ml.berkeley.edu/blog/posts/clip-art/">some blog articles</a> on the topic, I extended some code to give me more flexibility over the types of inputs and outputs I can use to alter the asthetics, styles and other aspects of the resulting visual products. Here I wanted to describe some approaches I've been using as well as the resulting products based on custom artwork produced by friends.</p>
<hr />
<p>Powered by models trained and released by <a href="https://openai.com/">OpenAI</a>, some early approaches, such as the <a href="https://openai.com/blog/dall-e/">DALL-E neural network models</a>, were created specifically to generate images from text (<a href="https://ml.berkeley.edu/blog/posts/vq-vae/">see explanation here</a>). In contrast, newer approaches are built on models that were designed to generalize classical image classification and caption generation tasks. Perhaps the most popular of these is the <a href="https://openai.com/blog/clip/">CLIP model</a>. In the process of being trained to solve tasks involving the relationship between images and natural language, the models learn about the complex relationships between the two modalities that can be revealed by trying various combinations of inputs and outputs. Given that these models were not originally designed to create visual art, the results of experiments can be both surprising and interesting - exactly the features that excite visual art creators and enthusiasts. </p>
<p>My use of the model involves training the last layer of the deep learning network model provided by CLIP, where the objective function is provided by any combination of image and text inputs. The total objective function is a simple linear combination of objective functions produced by each of the text and image inputs, and so in this way I can carefully choose inputs that might produce the images I think are interesting. Through a process of trial and error I've been playing with a number of approaches to image generation that I have found to produce meaningful differences.</p>
<h3>Text inputs only</h3>
<p>The simplest use of these models would start with random initialization and use a single text input to generate the objective function. For the examples below, I created gifs where each frame is two training iterations. You can see that they all start with random (but correlated) noise. These images are fairly sensitive to random noise since it is a highly nonconvex objective function.</p>
<p>Here are a few examples I created:</p>
<h5>"sunset in the city"</h5>
<p><img alt="sunset in the city" src="https://storage.googleapis.com/public_data_09324832787/mlart/text_only03-None-im-text0%3Dsunset_in_the_city-0_final.gif" /></p>
<p>This example shows how the model clearly has a sense of the set of textures that might be associted with a city at sunset. The consistent appearance of aligning (yet sometimes slightly offset) windows, glow that might appear as reflections of the sun, and even the colors of concrete and steel that you might find in a photo or painting of a cityscape. Keep in mind that the CLIP model used to generate all of these was trained on real images in databases, so it's no surprise that they capture some of the realism we'd see in a regular photo.</p>
<h5>"orcas in the north pole"</h5>
<p><img alt="orcas in the north pole" src="https://storage.googleapis.com/public_data_09324832787/mlart/text_only03-None-im-text0%3Dorcas_in_the_north_pole-4_final.gif" /></p>
<p>The first thing to note is that the white background and snowy trees clearly capture some aspects of winter in the North Pole. The "orcas" part of the image is less clear though - they appear as random blobs that emulate the curvature and black/white contrast of the animals. We also see small bodies of water spread throughout the image.</p>
<h5>"enter the matrix"</h5>
<p><img alt="enter the matrix" src="https://storage.googleapis.com/public_data_09324832787/mlart/text_only03-None-im-text0%3Denter_the_matrix-1_final.gif" /></p>
<p>This test clearly demonstrates that the training data used to generate the full CLIP model included images from popular movies like the matrix. In my exploration I found that a lot of other popular media is represented here as well.</p>
<h3>Stylized text inputs</h3>
<p>Now I will show a way we can use artifacts from the training data to produce stylized versions of the previous examples. I got the idea for this approach from <a href="https://ml.berkeley.edu/blog/posts/clip-art/">this blog article</a>, and extended it by further hypothesizing which kinds of data were included in the training data. The so-called "artifacts" in the training data are simply the source of the images. For instance, experimentation suggests that some of the training data may consist of images from Flickr, and by including "Flickr" in the text we can ask the model to re-create some stylistic elements from those photos. I also had success experimenting with Deviantart, Unreal Engine, etc. Similarly to our Matrix example, we can also use stylistic elements from certain types of media such as films by Studio Ghibli.</p>
<h5>"sunset in the city on Flickr"</h5>
<p><img alt="sunset in the city on Flickr" src="https://storage.googleapis.com/public_data_09324832787/mlart/stylized02-None-im-text0%3Dsunset_in_the_city_on_Flickr-1_final.gif" /></p>
<p>Here I re-used one of the previous examples but added the text "on Flickr" at the end. Instead of appearing like the side of buildings in the sunset, this image includes what appear to be power lines, traffic light poles, sides of cars, and even shapes that look suspiciously like people viewed from behind. The implications of these artifacts are really interesting. First, the inclusion of elements like power lines suggests that images from Flicker that are captioned with "city" are more likely to be photos from the ground than from the sky - it makes sense intuitively. Also note that it seems to carry realism styles into the image.</p>
<h5>"sunset in the city Artstation"</h5>
<p><img alt="sunset in the city Artstation" src="https://storage.googleapis.com/public_data_09324832787/mlart/stylized03-None-im-text0%3Dsunset_in_the_city_Artstation-0_final.gif" /></p>
<p>I tried "<a href="https://www.artstation.com">Artstation</a>" because it is another popular website for posting user-created art. Note tha variation in building design, the distribution of open space over the scene, and the human body-like objects. It is worth looking at the <a href="https://www.artstation.com">Arstation website</a> to give a sense of what images there look like.</p>
<h5>"sunset in the city Ghibli"</h5>
<p><img alt="sunset in the city Ghibli" src="https://storage.googleapis.com/public_data_09324832787/mlart/stylized03-None-im-text0%3Dsunset_in_the_city_Ghibli-1_final.gif" /></p>
<p>I thought this example was particularly cool because it shows how the addition of the text "Ghibli" brings both stylistic elements and actual objects from movies produced by Studio Ghibli into the scene. The sun appears as round globes instead of just reflections off of buildings - unsuprising given that real photographs can rarely capture the sun alongside an actual scene in the way that a cartoon could. Also note the grass and trees that don't appear in the other images. This fits neatly within the realm of the relationship between human technology and nature that we often see in Miyazaki films. The buildings here are not made of square edges as we saw in the last several examples, but rather have rounded roofs and are made of concrete instead of closely packed windows.</p>
<h5>"Angel's Landing Artstation"</h5>
<p><img alt="Angel's Landing Artstation" src="https://storage.googleapis.com/public_data_09324832787/mlart/Angels_Landing_artstation_4_iter1004.png" /></p>
<h2>Nonrandom initialization</h2>
<p>Until now we've been using random initialization to begin the model training, but the coolest outputs from these models come when we actually initialize parameters from an existing image. By giving it an initial image and some text, we can begin to add information to the existing photos which previously did not exist.</p>
<p>The simplest example I can show</p>
<h4>Freedom</h4>
<p>Initial image: <a href="https://storage.googleapis.com/public_data_09324832787/mlart/kiersten_freedom_painting_cropped.jpeg">"Freedom"</a></p>
<p>Text prompt: "freedom"</p>
<p><img alt="freedom" src="https://storage.googleapis.com/public_data_09324832787/mlart/freedom1.gif" /></p>
<h4>Transcendance</h4>
<p>Initial image: "<a href="https://storage.googleapis.com/public_data_09324832787/mlart/varun_painting_1.jpeg">peace and transcendance</a>"</p>
<p>Text prompt: "transcendance"
<img alt="Transcendance" src="https://storage.googleapis.com/public_data_09324832787/mlart/varun1.gif" /></p>
<h4>Sunset in the mountains</h4>
<p>Initial image: stock photo</p>
<p>Text prompt: "Sunset in the mountains"</p>
<p><img alt="Sunset in the mountains" src="https://storage.googleapis.com/public_data_09324832787/mlart/sunset1.gif" /></p>
<h2>Image-based objective functions</h2>
<p>The real advantage of using these CLIP models is that we can add images to generate the objective function.</p>
<h3>Symbols</h3>
<p>The image-based objective functions can add complex textures to even the simplest initial images. I have only tried it on a few starting images so far though.</p>
<h4>Om</h4>
<p>Text prompt: "peace and love"</p>
<p>Image prompt: <a href="https://www.artstation.com/artwork/mDWZxY">"The Aluren Forest"</a></p>
<p>Initial image: <a href="https://storage.googleapis.com/public_data_09324832787/mlart/Aum_Om_black.png">Om symbol</a></p>
<p>"Peace and Love Deviantart"</p>
<p><img alt="peace and love Om" src="https://storage.googleapis.com/public_data_09324832787/mlart/peace_and_love_deviantart_step0.05_0_final.png" /></p>
<h3>Paintings</h3>
<h4>"Our Garden"</h4>
<p>Original image: "<a href="https://storage.googleapis.com/public_data_09324832787/mlart/angela_garden1.jpg">Angela's garden</a>"</p>
<p>Image prompt: <a href="https://www.artstation.com/artwork/mDWZxY">"The Aluren Forest"</a></p>
<p>Text prompt: "garden"</p>
<p><img alt="garden" src="https://storage.googleapis.com/public_data_09324832787/mlart/angela04_garden-garden--angela_garden1-angela_garden1.sunset_forest-0_final.png" /></p>
<p>Text prompt: "lush green forest with flowers"</p>
<p><img alt="garden" src="https://storage.googleapis.com/public_data_09324832787/mlart/angela04_garden-lush_green_forest_with_flowers--angela_garden1-angela_garden1.sunset_forest-0_final.png" /></p>
<p>Text prompt: "flowers at sunset"</p>
<p><img alt="flowers at sunset" src="https://storage.googleapis.com/public_data_09324832787/mlart/angela04_garden-flowers_at_sunset--angela_garden1-angela_garden1.sunset_forest-0_final.png" /></p>
<p>Animation:</p>
<p><img alt="flowers at sunset animation" src="https://storage.googleapis.com/public_data_09324832787/mlart/angela04_garden-flowers_at_sunset--angela_garden1-angela_garden1.sunset_forest-0_final.gif" /></p>
<h4>Campfire in the night</h4>
<p>Original image: <a href="https://storage.googleapis.com/public_data_09324832787/mlart/sujaya_camping.jpg">campfire scene</a></p>
<h5>"campfire starry night"</h5>
<p><img alt="sujaya" src="https://storage.googleapis.com/public_data_09324832787/mlart/sujaya12_campfire_starrynight-campfire_in_starry_night--sujaya_camping-sujaya_camping.starry_night-0_final.png" /></p>
<h5>"forest at sunset"</h5>
<p><img alt="sujaya" src="https://storage.googleapis.com/public_data_09324832787/mlart/sujaya13_campfire_sunset-forest_at_sunset--sujaya_camping-sujaya_camping.sunset_forest-0_final.png" /></p>
<h5>"Stars in the Sky Ghibli"</h5>
<p><img alt="sujaya" src="https://storage.googleapis.com/public_data_09324832787/mlart/sujaya10_campfire-stars_in_the_sky-ghibli-sujaya_camping-sujaya_camping.cosmos_stock-0_final.png" /></p>
<h3>Time-dynamic animations</h3>
<p>So far I have shown gifs that show each training iteration as the model converges. In addition to giving us an intuition as to how the model is training, it feels like it brings the scenes to life - objects and stylistic elements are constantly moving and changing slightly. This gave me the inspiration to try changing the text and image objective functions while the model trains.</p>
                    </div>
                </div>
            </div>
        </article>
        <!-- Footer-->
        <footer class="border-top">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <ul class="list-inline text-center">
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-facebook-f fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                        </ul>
                        <div class="small text-center text-muted fst-italic">Copyright &copy; Devin J. Cornell 2021</div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="/js/blog.js"></script>
    </body>
</html>
