<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Devin J. Cornell: ZODS Leftovers?</title>
        <link rel="icon" type="image/x-icon" href="/assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <!--<script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js" crossorigin="anonymous"></script>-->
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="/css/blog.css" rel="stylesheet" />
        <style>
            pre {
                background-color: #ececec;
                border-radius: 4px;
                padding: 10px;
            }
        </style>
    </head>
    <body>
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
            <div class="container px-4 px-lg-5">
                <a class="navbar-brand" href="/">devinjcornell.com</a>//<a class="navbar-brand" href="/blog">Data Science Blog</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto py-4 py-lg-0">
                        <!--<li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="index.html">Home</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="about.html">About</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="post.html">Sample Post</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="contact.html">Contact</a></li>-->
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Page Header-->
        <header class="masthead" style="background-image: url('assets/img/post-bg.jpg')">
            <div class="container position-relative px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-8">
                        <div class="post-heading">
                            <h1>ZODS Leftovers?</h1>
                            <h2 class="subheading">random scraps.</h2>
                            <span class="meta">
                                Posted by
                                <a href="/">Devin J. Cornell</a>
                                on May 28, 2023
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <!-- Post Content-->
        <article class="mb-4">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <h1>VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV</h1>
<p>Are data frames too flexible?
Custom types VS data frames: choosing the right data structures for your project.</p>
<p>Over the last decade of teaching and reading about data science practices, I have seen data frame-oriented tools like Pandas and tidyverse become essential reading materials for any student of data science. While these tools are undoubtedly valuable, they make data pipelines error-prone and difficult to maintain as projects grow and requirements change. In my recent blog article, I discuss some of the challenges with using them in larger projects and provide some alternative patterns that are more effective in many scenarios.</p>
<p>Jupyter and RStudio markdown because they allow for quick experimentation and enable near-instant feedback. Expansive packages like Pandas and tidyverse are becoming essential material, and students often engage with them before they even understand the language they are built in (me too, sometimes). There is no doubt that these are powerful and useful tools, but I argue that we should return to the basics if we want to create maintainable data pipelines.</p>
<h1>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</h1>
<h4>VVVVVVVVVVV ALL EXPERIMENTAL VVVVVVVVVVV</h4>
<p>They do not appear in your code except at the type of conversion or enforcement, or, even more sketchy. </p>
<p>but my concern is that the types of individual columns are never known by your interpreter until you actually run the code. While you may specify and enforce column  </p>
<p>or contained elements are not known by your interpreter</p>
<p>by the reader or static analyzer prior to runtime. You can specify and enforce the types of columns in dataframes, but your interpreter or analyzer never actually</p>
<p>they are not probject-specific - that is, they do not enforce structures that are relevant to the specific project for which the pipeline is being built. When you read a dataframe from a csv file, for example, you can specify the code </p>
<p>As such, simply knowing the type of an object does not give us insight into the representations that appear in the pipeline. </p>
<p>Dataframes maintain types for the columns they maintain, however, you cannot see the types unless you do some introspection into the </p>
<p>For a further elaboration on what I mean by adding more structure, see </p>
<p>. As an example, if you read a csv file as a dataframe, consider creating a class definition that represents a single row of that dataframe and include the code to parse that data within the same class, as well as any methods that operate on that class' data. Then encapsulate those objects into collections in which you can build additional methods for parsing, grouping, filtering, or transforming collections/lists/etc. By defining classes explicitly, your analyzer knows which attributes and methods are available on that object at any point in time. Avoid using lists of dictionaries or other datastructures without defined types, as they have the same pitfalls as dataframes.</p>
<h4>^^^^^^^^^^^^^^^^^</h4>
<p>Hiiiii^[Note that dataframes themselves are types and their columns have specific types within those objects, but the defining characteristic is that the interpreter or analyzer cannot infer those types without looking at the behavior of the functions or scripts used to produce it (which they often do not). They are types within the underlying package code, but they are not considered as types within the language itself. If you build your pipelines using functions that both accept and return dataframes, you do not know the structure of the new dataframe unless you look at the code used to transform it. In contrast, if you define custom types for the input and output data, you can know without looking at the]</p>
<h4>^^^^^^^^^^^^^^^^^</h4>
<h2>Data pipelines: separating the "what" from the "how"</h2>
<p>The topmost path in the figure shows the case where we do not keep track of the structure of the input or intermediate data in our code explicitly (imagine using a list of dictionaries or a dataframe read from a csv file), wheras in the bottom pipeline we represent them as objects A, B, and C explicitly in our code. The idea is that pipelines with explicit references to data structure in the code make it easier to understand what each transformation is doing - in theory, we (and the static analyzer in your IDE) could understand the entire pipeline without ever <em>running</em> our code.</p>
<h4>^^^^^^^^^^^^^^^^^^^</h4>
<h2>Debugging Pipelines</h2>
<p>Let us explore the case where you do not use custom data objects, and instead use dataframes or lists of dictionary/collections, or some other non-explicit data structures. As a hypothetical, say you are seeing a potential issue in your final data structure - a figure, let's say - and you want to investigate why you observe a given value. First, you hypothesize that the issue may have been with function/script 2, and so we first need to understand the structure of the intermediary data which it transformed. There are three approaches to understanding the intermediate data structure when we have not been explicit in our code: </p>
<ol>
<li>
<p>remember the structure of that data - generally a bad thing to rely on in software design because you may be looking at this years later or someone else may be looking at it;</p>
</li>
<li>
<p>run the first pipeline component and use some runtime introspection tool (breakpoints, print statements, debuggers, etc) to look at the data - possible but clunky and time-consuming; or </p>
</li>
<li>
<p>do some mental bookkeeping to trace the original input data (which may also require introspection) through the pipeline - also a time-consuming activity. </p>
</li>
</ol>
<p>None of these options look good - the best scenario is option 3, and even that is only viable if you know both the structure of the input data and are okay reading through the logic up until that point. Unfortunately, debugging or changing intermediary stages of your data pipeline will happen all the time - this can create some big problems as your project grows and your requirements change.</p>
<p>What is the problem with running the code in real time? In my experience, this simply takes a lot longer than keeping track of the code itself (either reading it or using a static analyzer) when it comes to large data pipelines. Each step or set of steps in your pipeline are expensive and probably time-consuming. To make it easier, you might optimize the pipeline by storing intermediary steps (RData or pickle files) so you can load them into separate notebooks more quickly, but this optimization is time-consuming and would need to be done every time you set out to work. In software engineering, it is generally far better to detect any problems without needing to actually run your code.</p>
<p>In the case where you represent the structure of your data as part of the code itself (i.e. use classes/structs to define intermediate structures), however, you (and your compiler/static analyzer) know the structure of the data at every stage of the pipeline because it is explicitly defined. From this alone you know not only that your data will appear in the specified formats (providing some gaurantees), but also that the role of that particular function/script is to convert data of type B to type C. In this case, the pipeline issue will be much easier to identify.</p>
<h3>Case Against Dataframes</h3>
<p>While dataframes are important data structures that a large suite of languages and packages have been built around, I have two primary concerns about using them as a central feature of your data pipelines: (1) all of the problems we observe above, and (2) they are often the wrong tools for the job (performance-wise) - even though they may be fine for many tasks involving small datasets.</p>
<p>The first point appears to be acceptable for many data scientists given that it is common to use Jupyter or R Markdown notebooks to write large portions of code. Except in initial development or in your toplevel scripts, I recommend using project file structures that are recommended for your language of choice - in Python, this means separating functions and classes (including the data containers) into modules, but there are equivalent recommended project structures for most langauges. Data science projects in particular tend to grow in scope or change in structure often, so modular project structures are especially important. The more complex your code becomes, the more important this is.</p>
<p>More concretely, lets refer to the iris example dataset we loaded. We access a column of that data using a subscript or as a property of the dataframe (although be careful with the latter):</p>
<pre><code>iris_df['species']
iris_df.species
</code></pre>
<p>Or, similarly in R:</p>
<pre><code>iris_df[:,'species']
iris_df$species
</code></pre>
<p>The problem with this is that you have no gaurantees that this property exists with this name in your input data. Even though the R and Python versions are both written as if the columns are object properties, they are not - they are simply syntactic sugar used to make it feel like they are - the reader, and your static analyzer, cannot gaurantee they exist except in runtime.</p>
<p>Sure, you could run a verification or transformation function that selects/orders columns and does some validation, but this code is implemented as part of the script loading the data, not in the definition of the data itself.</p>
<pre><code>iris_df = iris_df[['sepal_length', 'sepal_width', 'species']]
assert((iris_df['sepal_length']&gt;0).sum() == iris_df.shape[0])
</code></pre>
<p>In short, you have no gaurantees that a property will exist in runtime (assuming, as in the case of weakly typed languages, you follow the hints/gaurantees that you yourself provided).</p>
<p>I a also caution against using dataframes for performance reasons. Traditional data structures like <code>set</code>s, <code>dict</code>s (any kind of map), or <code>list</code>s (arrays) in Python all have implementations that are akin to different tasks. If you want to store an unordered set of unique elements, use a <code>set</code> object. If you want to perform lookups from a string or other hashable object to another object directly (i.e. in O(1) time), use a <code>dict</code>. If you want to store an ordered sequence of elements that can be indexed by their order in the sequence, use a <code>list</code>.</p>
<p>As an example of this issue, lets say that you want to perform a join on a two unindexed dataframes A with <code>n</code> rows and B with <code>m</code> rows that have a many-to-one relationship: that is there are zero or more rows of A that correspond to a single row of B. Because dataframe columns are implemented as sequential elements, the time it will take for this join to complete will be proportional to <code>n*m</code>: you would iterate through every element of A to find the associated row in B (note that this is worst case: <code>O(n*m)</code>). This is the wrong tool for the job because it could be done faster with a dictionary or hash map that allows you to look up the associated row in dataframe B instantly, so the time would be proportional to <code>n</code> (even if you have to create the dictionary first, you end up with <code>n + m ~ O(n)</code>).</p>
<p>Note that indexing dataframes is an operation that would allow for faster joins (typically through binary search, which is <code>O(n*log(m))</code>), indexing and multiindexing implementations are somewhat clunky to use and often involve explicit indexing between transformations. It is worth noting they exist, though I don't often see them used.</p>
<p>Dataframes can also be more memory intensive because joins and most other operations often create copies of data - even when it may be unnessecary. Be wary of this as your dataset gets larger.</p>
<p>As an alternative, I recommend creating an object to represent each row of your dataset, and parsing each row using a factory method - I will give some examples later.</p>
<h3>Nested Iterables are Also Bad</h3>
<p>It may also be tempting to use raw nested iterables like <code>set</code>s, <code>dict</code>s, or <code>list</code>s either because they follow directly from the structure of the input data (especially json data) or because they solve the second issue I have with dataframes - you can use the right tool for the job. My main concern with these structures is that they can get very complicated with high levels of nesting and requre missing data/error handling at every point of usage.</p>
<p>As another example, let us consider a json dataset parsed as a list of dictionaries with properties associated with irises. The type hint for the parsed structure would be <code>typing.List[typing.Dict[str, typing.Union[float, str]]]</code>.</p>
<pre><code>[
    {
        "sepal_length": 5.1, 
        "sepal_width": 3.5, 
        "species": "setosa"
    }, 
    {
        "sepal_length": 4.9, 
        "sepal_width": 3.0, 
        "species": "setosa"
    },
    ...
]
</code></pre>
<p>Say we want to get the average petal length for irises in our dataset, so we do this:</p>
<pre><code>import statistics
statistics.mean([iris['petal_length'] for iris in irises])
</code></pre>
<p>The problem here is the same as that of selecting columns in dataframes: we have no gaurantees that each iris will include a member called 'petal_length' until runtime. We can't know it exists unless we recall the data being passed in, and static analysis tools cannot help us.</p>
<p>The solution to these problems would again be to create a class representing a single Iris object and parsing them into a list of these objects. Even better, we could encapsulate the sequence of irises to add additional convenience. I will show some examples in the next sections.</p>
<h1>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</h1>
                    </div>
                </div>
            </div>
        </article>
        <!-- Footer-->
        <footer class="border-top">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <ul class="list-inline text-center">
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-facebook-f fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                        </ul>
                        <div class="small text-center text-muted fst-italic">Copyright &copy; Devin J. Cornell 2021<br/><hr/></div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="/js/blog.js"></script>
    </body>
</html>