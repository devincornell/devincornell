<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Devin J. Cornell: Are dataframes too flexible?</title>
        <link rel="icon" type="image/x-icon" href="/assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <!--<script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js" crossorigin="anonymous"></script>-->
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="/css/blog.css" rel="stylesheet" />
        <style>
            pre {
                background-color: #ececec;
                border-radius: 4px;
                padding: 10px;
            }
        </style>
    </head>
    <body>
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
            <div class="container px-4 px-lg-5">
                <a class="navbar-brand" href="/">devinjcornell.com</a>//<a class="navbar-brand" href="/blog">Blog</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto py-4 py-lg-0">
                        <!--<li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="index.html">Home</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="about.html">About</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="post.html">Sample Post</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="contact.html">Contact</a></li>-->
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Page Header-->
        <header class="masthead" style="background-image: url('assets/img/post-bg.jpg')">
            <div class="container position-relative px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-8">
                        <div class="post-heading">
                            <h1>Are dataframes too flexible?</h1>
                            <h2 class="subheading">Using custom types in your data pipelines.</h2>
                            <span class="meta">
                                Posted by
                                <a href="/">Devin J. Cornell</a>
                                on May 28, 2023
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <!-- Post Content-->
        <article class="mb-4">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <p>Dataframe interfaces are useful because they are so flexible: filtering, mutatating, selecting, and grouping functions have simple interfaces and can be chained to perform a wide range of transformations on tabular data. The cost of this flexibility, I argue, is that your data pipelines are less readable, more difficult to maintain, and more error prone. Instead, I argue that it is better to use more explicit data structures like classes or structs with fixed attributes, specific methods for construction, and specific methods for transformation/analysis. </p>
<p>Over the last decade of teaching and reading about data science practices, I have seen a shift in the way that students are learning. Students start learning with tools like Jupyter and RStudio markdown because they allow for quick experimentation and enable near-instant feedback. Expansive packages like Pandas and tidyverse are becoming essential material, and students often engage with them before they even understand the language they are built in (me too, sometimes). There is no doubt that these are powerful and useful tools, but I argue that we should return to the basics if we want to create maintainable data pipelines.</p>
<p>In this article, I will contrast dataframes with what I will refer to as custom data types, or data types that you define yourself as part of your data pipeline. Using custom data types means you explicitly define the structure of a particular dataset in your code before you actually attempt to use it. While I recognize that dataframes do have strengths, I argue that custom data types are far better options as your projects grow and become more complex, and will be especially important moving forward as the average developer uses more advance static analysis or other assistance tools for writing code more efficiently.</p>
<h2>Data Structures and Pipelines</h2>
<p>A <em>data pipeline</em> is a series of sequential steps for changing data from one format to another - the essential core of all data science projects. Maybe you want to visualize some variables from a CSV file in a 2-dimensional plot, produce a statistical model to capture trends of Tweets in json format, or even build a classifier to identify cats from an image training data set. In each of these cases, the data pipeline simply describes the set of transformations and intermediary representations needed to produce the final form from the given input data.</p>
<p>I use the term <em>data structures</em> to describe the intermediary representations of data in these pipelines. Essentially, this means the format in which your data is represented in your computer system and the interface (API) in your code used to access and manipulate it. Any <a href="https://ocw.mit.edu/courses/6-851-advanced-data-structures-spring-2012/">computer science cirriculum</a> includes an analysis of common data structures and optimal algorithms for manipulating and analyzing them, but they play what is perhaps a more important role in software engineering of data pipelines: they can be used to make your data pipelines easier to understand, less error-prone, and more efficient to maintain.</p>
<div id="illustration">.</div>

<h3>Illustration of Data Pipeline</h3>
<p>Below I created a simple diagram with two linear data pipelines depicting the transformation of the input data into an intermediate data structure which is changed into the final data to be shared with the customer (a table or figure, let's say). </p>
<p><img alt="data science pipeline overview" src="https://storage.googleapis.com/public_data_09324832787/pipeline_structures.png" /></p>
<p>Anything represented in the computer is a data structure. For instance, the input could be a CSV file that you first read as a dataframe (intermediate data structure), average a set of values to produce another dataframe (another intermediate structure), and then convert to a figure which you then display on your screen (the final data structure). Or, for instance, the input could be a set of images and classification labels and the output could be a machine learning model trained to identify the classes. In this way, the pipeline captures the essence of any data analysis project. </p>
<div id="features">.</div>

<h3>Features of Data Structures</h3>
<p>I will focus on three aspects of data structures which are relevant for design patterns I will discuss. They exist in almost every type of data structure, and the key is in where and how they appear in your code.</p>
<ol>
<li>
<p><strong>Properties or attributes.</strong> Data structures often include sets of properties, attributes, or features that are associated with a single element - the "what" of your data pipelines. These might be represented as columns in dataframes where each row is an element, attributes in custom types, or as separate variables. They can be defined at instantiation (point where the structures are created) or later added, modified, or removed throughout your pipeline. The data is called <em>immutable</em> if it cannot be changed, and <em>mutable</em> otherwise. </p>
</li>
<li>
<p><strong>Construction methods.</strong> Functions used to create and instantiate data structures are called construction methods. These functions are critical because they include at least some, if not all, information about which data will be contained within the structure. As such, the function signature should tell the reader (and compiler or static analyzer) a lot about what type of data is being represented. These methods can appear in your code as class methods, functions, or entire scripts. As an example, they may include the code used to parse json or csv data into a data object.</p>
</li>
<li>
<p><strong>Transformation methods.</strong> These are the methods which actually convert your data structures from one form to the next - the "how" of your data pipelines. They may appear in your code as class methods, functions, or entire scripts. Common transformations might include filtering, summarizing, or normalizing your data. This is a more general case than construction methods, which could also be considered as transformation methods.</p>
</li>
</ol>
<p>Next I will use these three features as comparison points.</p>
<div id="attributes">.</div>

<h2>Comparison of Data Structures</h2>
<p>I will now compare dataframes with custom data types using Python examples, although I believe these points apply to approaches and strategies in many different languages. Specifically, I will use the classic Iris datasets loaded from the seaborn package.</p>
<p>In Python, we would load the Iris dataset as a dataframe using the following code (note that seaborn is only used to load the data).</p>
<pre><code>import seaborn
import pandas as pd

iris_df = seaborn.load_dataset("iris")
iris_df.head()
</code></pre>
<p>The dataframe looks like this.</p>
<pre><code>sepal_length  sepal_width  petal_length  petal_width species
0           5.1          3.5           1.4          0.2  setosa
1           4.9          3.0           1.4          0.2  setosa
2           4.7          3.2           1.3          0.2  setosa
3           4.6          3.1           1.5          0.2  setosa
4           5.0          3.6           1.4          0.2  setosa
</code></pre>
<p>For example purposes, I'll start with a list of dictionary objects representing irises - the most basic built-in data structures in Python. I'll use the <code>DataFrame.to_dict</code> method to accomplish this.</p>
<pre><code>iris_data = iris_df.to_dict(orient='records')
</code></pre>
<p>The first few elements of this data looks like the following:</p>
<pre><code>[
    {
        'sepal_length': 5.1,
        'sepal_width': 3.5,
        'petal_length': 1.4,
        'petal_width': 0.2,
        'species': 'setosa'
    },
    {
        'sepal_length': 4.9,
        'sepal_width': 3.0,
        'petal_length': 1.4,
        'petal_width': 0.2,
        'species': 'setosa'
    },
    ...
]
</code></pre>
<h3>1. Properties or Attributes of Data Structures</h3>
<h4>The dataframe approach</h4>
<p>Dataframes typically represent data attrbutes as columns, and each column is represented as an array of an internal type, rather than a type within the langauge. Python, for instance, implements int and float objects, but Pandas dataframes include more specific types like 64 bit integers and floating point numbers (following NumPy arrays) that do not appear in the Python specification.</p>
<p>In Python, you would access columns using the following notation.</p>
<pre><code>iris_df['species']
iris_df.species
</code></pre>
<p>And subsets of columns in Python can be extracted using the following.</p>
<pre><code>iris_df = iris_df[['sepal_length', 'sepal_width', 'species']]
</code></pre>
<p>The issue I have with these methods for selecting attributes is that you do not actually know if the columns you describe here actually exist within the data set unless you know both the initial dataset being loaded from disk and every subsequent transformation that happens throughout your pipeline until the point where you access it. Nothing about the existence of the dataframe object gaurantees the existence of those attributes, so your IDE or static analyzer do not know whether there is an error here or not, and it will not be able to provide suggestions for autocomplete.</p>
<h4>Custom type approach</h4>
<p>As an alternative, consider using custom data object types with a fixed set of specified attributes to represent your data. While more code is needed to create the types, the mere existence of the object comes with gaurantees about which attributes they contain. You do not need to understand the transformation used to create the object to know that the attributes will exist.</p>
<p>In most languages, I recommend creating classes or struct types to represent you data. In Python, you can use dataclasses or the attrs package to easily create objects that are meant to store data. The following class represents a single Iris object.</p>
<pre><code>import dataclasses

@dataclasses.dataclass
class IrisEntry:
    sepal_length: float
    sepal_width: float
    petal_length: float
    petal_width: float
    species: str
    ...
</code></pre>
<p>The dataclasses module creates a constructor where all these values are required, so you may instantiate an IrisEntry like the following:</p>
<pre><code>IrisEntry(1.0, 1.0, 1.0, 1.0, 'best_species')
</code></pre>
<p>You can then store these objects in collections, and I recommend either encapsulating those collections or at least extending an existing collection type to make the intent clearer to the reader - especially in weakly typed langauges. In Python, you might extend a List using the following.</p>
<pre><code>class Irises(typing.List[IrisEntry]):
    ...
</code></pre>
<p>The benefit of defining these types is that it should be obvious to any reader which properties are associated with witch types of data. If you try to access an attribute that does not exist, you will see an exception, and furthermore your static analyzer or IDE will be able to autocomplete or let you know when you make an error before you ever run your code. You are making a gaurantee that every time an object like this exists, it will have these attributes.</p>
<p>One final note here - in more weakly typed languages like Python or R, I recommend creating immutable types, or objects that cannot be modified or extended after construction. This restriction will make for cleaner methods/functions throughout your pipeline.</p>
<h3>2. Constuction Methods</h3>
<p>Construction methods are critical for understanding your data pipeline because they often reveal which data the structure will encapsulate and the operations needed to encapsulate it. </p>
<p>For example purposes, lets try deconstructing the original iris dataframe into Python dictionaries and recreate a dataframe from there. First I'll use the <code>to_dict</code> method to create the list of dictionaries.</p>
<pre><code>iris_data = iris_df.to_dict(orient='records')
</code></pre>
<p>The first few elements of this data looks like the following:</p>
<pre><code>[
    {
        'sepal_length': 5.1,
        'sepal_width': 3.5,
        'petal_length': 1.4,
        'petal_width': 0.2,
        'species': 'setosa'
    },
    {
        'sepal_length': 4.9,
        'sepal_width': 3.0,
        'petal_length': 1.4,
        'petal_width': 0.2,
        'species': 'setosa'
    },
    ...
]
</code></pre>
<p>And now let us create a function to convert this data from a list of dictionaries to a dataframe. We can do this easily using the <code>DataFrame.from_records</code> method, again demonstrating the flexibility and power of dataframe-oriented packages.</p>
<pre><code>def make_iris_dataframe(iris_data: typing.List[typing.Dict[str, typing.Union[float, str]]]) -&gt; pd.DataFrame:
    return pd.DataFrame.from_records(iris_data)
</code></pre>
<p>Imagine you have a data pipeline where this function is the first step, and one day the data source changes the "species" attribute to be "type". This example function would not raise any exceptions or flags, but instead propogate this change further in your data pipeline such that you only know it would be broken when you try to access the column with the old name later in the pipeline. When the downstream function raises an exception, you will not immediately know whether it was because the original dataset changed or if it was an error in that first function. </p>
<p>The common solution to this problem is to add a standard column selection that would fail if a column has been renamed, but again it requires us to know the content of the function and also remember to build this code into any function that makes the dataframe from source data. To test whether the function worked, you will need to examine the structure of the dataframe.</p>
<pre><code>def make_iris_dataframe_standardize(iris_data: typing.List[typing.Dict[str, typing.Union[float, str]]]) -&gt; pd.DataFrame:
    df = pd.DataFrame.from_records(iris_data)
    return df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
</code></pre>
<p>A principle of good design is that your system should fail as early in the pipeline as possible so that you can isolate any issues at the point of the failure rather than to downstream functions which rely on them.</p>
<p>As an alternative, you can consider using a static factory method (see the <code>classmethod</code> decorator in Python) on a custom type to contain code needed to create the object from various sources. This example shows code needed to create a <code>IrisEntry</code> object from a single row of the iris dataframe.</p>
<pre><code>@dataclasses.dataclass
class IrisEntry:
    sepal_length: float
    sepal_width: float
    petal_length: float
    petal_width: float
    species: str

    @classmethod
    def from_series(cls, row: pd.Series):
        return cls(
            sepal_length = row['sepal_length'],
            sepal_width = row['sepal_width'],
            petal_length = row['petal_length'],
            petal_width = row['petal_width'],
            species = row['species'],
        )
</code></pre>
<p>And the collection type could tie it together by calling the static factory method on each row of the dataframe.</p>
<pre><code>class Irises(typing.List[IrisEntry]):
    @classmethod
    def from_iris_df(cls, iris_df: pd.DataFrame):
        return cls([IrisEntry.from_series(row) for ind, row in iris_df.iterrows()])
</code></pre>
<p>One could imagine creating similar static factory methods for constructing this data structure from any type of input data - not just dictionaries or dataframes.</p>
<h3>3. Transformation Methods</h3>
<p>Methods that actually transform data from one type to another will probably make up the majority of the work in your data pipeline. Of course, regaurdless of the implementation and language, dataframes have a wide range of standard transformation methods such as mutations, filters, and aggregations that will make up the majority of your workflows. Throughout your pipeline, you will probably at least group application-specific transformations into functions, or operations that operate on dataframes with a specific set of columns and types - the iris dataframe, for instance.</p>
<h5>Element-wise Transformations</h5>
<p>The simplest transformation is where each element (or row in the dataframe) can be transformed into a new type of data. For example, lets say you want to calculate the sepal and petal areas of each iris. The cleanest way to do this would be to create a new dataframe, so you could create a dataframe like the following.</p>
<pre><code>def calc_iris_area(iris_df: pd.DataFrame) -&gt; pd.DataFrame:
    return pd.DataFrame({
        'sepal_area': iris_df['sepal_length'] * iris_df['sepal_width'],
        'petal_area': iris_df['petal_length'] * iris_df['petal_width'],
        'species': iris_df['species'],
    })
</code></pre>
<p>Alternatively you could choose to modify the original dataframe in-place - this is less clean and could lead to further downstream errors, but it may be more efficient in some cases.</p>
<pre><code>def calc_iris_area_inplace(iris_df: pd.DataFrame) -&gt; pd.DataFrame:
    iris_df['sepal_area'] = iris_df['sepal_length'] * iris_df['sepal_width']
    iris_df['petal_area'] = iris_df['petal_length'] * iris_df['petal_width']
    return iris_df
</code></pre>
<p>You could even return a subset of the columns as a view, which could lead to slightly less risky workflows.</p>
<pre><code>    ...
    return iris_df[['sepal_area', 'petal_area', 'species']]
</code></pre>
<p>This again has the same risks as the constructor methods - the types alone do not really give us a sense of what the transformationw will be, becasue both the inputs and outputs are dataframes. We do not really even know if the same dataframe is being returned.</p>
<p>Alternatively, try creating a new object type to represent this new intermediary step in your pipeline. You can again use static factory methods that do a little of the work needed to create the object, although more complicated logic may be better contained elsewhere.</p>
<pre><code>@dataclasses.dataclass
class IrisArea:
    sepal_area: float
    petal_area: float
    species: str

    @classmethod
    def calc_from_iris(cls, iris: IrisEntry):
        return cls(
            sepal_area = iris.sepal_length * iris.sepal_width, 
            petal_area = iris.petal_length * iris.petal_width, 
            species = iris.species,
        )
</code></pre>
<p>You could even call this method back from the original <code>IrisEntry</code> object if you'd like to make a simpler high-level interface. Then you could use <code>IrisEntry.calc_area()</code> to compute area instead of <code>IrisArea.calc_from_iris(iris_entry)</code>.</p>
<pre><code>class IrisEntry:
    ...        
    def calc_area(self):
        return IrisArea.calc_from_iris(self)
</code></pre>
<p>The collection type would simply wrap it, as shown before.</p>
<pre><code>class IrisAreas(typing.List[IrisArea]):
    @classmethod
    def calc_from_irises(cls, irises: Irises):
        return IrisAreas([IrisArea.calc_from_iris(ir) for ir in irises])
</code></pre>
<p>To make the API easier, simply call that method from a new method in the <code>Irises</code> class as we did before.</p>
<pre><code>class Irises(typing.List[IrisEntry]):
    ...        
    def calc_areas(self):
        return IrisAreas.calc_from_irises(self)
</code></pre>
<p>The interface for working with these types would look like the following:</p>
<pre><code>irises = Irises.from_iris_df(iris_df)
iris_areas = irises.calc_area()
</code></pre>
<h5>Filtering and Aggregating</h5>
<p>In your pipeline, you will likely want to create transformation functions for filtering and aggregating that reference specific columns by names. These are two examples of such functions for dataframes, that have all the aforementioned readibility problems. That said, they are very compact and somewhat easy to read.</p>
<pre><code>def filter_lower_sepal_quartile(area_df: pd.DataFrame) -&gt; pd.DataFrame:
    v = area_df['sepal_area'].quantile(0.25)
    return area_df.query(f'sepal_area &gt; {v}')

def av_area_by_species(area_df: pd.DataFrame) -&gt; pd.DataFrame:
    '''Average iris areas by species.'''
    return area_df.groupby('species').mean().reset_index(inplace=False, drop=False)
</code></pre>
<p>The interface would then look like the following.</p>
<pre><code>area_df = calc_iris_area(iris_df)
filtered_area_df = filter_lower_sepal_quartile(area_df)
area_by_species = av_area_by_species(filtered_area_df)
</code></pre>
<p>In the custon-type approach, you would attach these functions as methods to your object classes. Notice that grouping and averaging are combinations of two functions here, and the returned value is a mapping from the species type to <code>IrisArea</code> objects (which can then retain their own methods). </p>
<pre><code>class IrisAreas(typing.List[IrisArea]):
    ...        
    def av_area_by_species(self) -&gt; typing.Dict[str, AverageIrisArea]:
        return {spec: areas.average() for spec, areas in self.group_by_species().items()}

    def average(self) -&gt; AverageIrisArea:
        return IrisArea(
            sepal_area = sum([ia.sepal_area for ia in self])/len(self),
            petal_area = sum([ia.petal_area for ia in self])/len(self),
            species = self[0].species,
        )

    def group_by_species(self):
        species_areas: typing.Dict[str, IrisAreas] = dict()
        for a in self:
            species_areas.setdefault(a.species, self.__class__())
            species_areas[a.species].append(a)
        return species_areas
</code></pre>
<p>And the high-level interface for these will look like the following.</p>
<pre><code>irises = Irises.from_iris_df(iris_df)
iris_areas = irises.calc_areas()
filtered_iris_areas = iris_areas.filter_lower_sepal_quartile()
averaged_iris_areas = filtered_iris_areas.av_area_by_species()
</code></pre>
<h5>Plotting Interfaces</h5>
<p>As a special case of these strategies, you may also want to implement plotting functions or objects as part of your pipelines. The dataframe approach is again a simple function that returns, in this case, a plotly object.</p>
<pre><code>import plotly.express as px
def plot_sepal_area(areas_by_species: pd.DataFrame) -&gt; pd.DataFrame:
    '''Plot average sepal area by species.'''
    return px.bar(areas_by_species, x='species', y='sepal_area')
</code></pre>
<p>When using custom types, I recommend creating an additional custom type that contain methods for plotting this particular data in any number of ways. This <code>IrisAreaPlotter</code> does a transformation from averaged <code>IrisArea</code> objects into a dataframe that plotly uses for plotting (a necessary step for any plotting method used here).</p>
<pre><code>@dataclasses.dataclass
class IrisAreaPlotter:
    iris_area_df: pd.DataFrame

    @classmethod
    def from_area_averages(cls, area_by_species: typing.Dict[str, IrisArea]):
        df = pd.DataFrame([dataclasses.asdict(a) for a in area_by_species.values()])
        return cls(df)

    def bar(self):
        return px.bar(self.iris_area_df, x='species', y='sepal_area')
</code></pre>
<p>You'd access those methods using this pattern.</p>
<pre><code>iris_plotter = IrisAreaPlotter.from_area_averages(averaged_iris_areas)
iris_plotter.bar()
</code></pre>
<p>Or, with additional changes, you could access it using <code>averaged_iris_areas.plot.bar()</code> or something similar.</p>
<h3>Comparison Summary</h3>
<p>Where the strengths of working with dataframes is that you can produce compact code by taking advantage of powerful methods built into existing packages, the weakness is that your pipeline codebase will be more difficult to organize and your IDE assistants (including AI-based solutions) will not be able to identify issues until you actually run your code. </p>
<div id="conclusions">.</div>

<h2>Conclusions</h2>
<p>Finally, it is worth considering these two data pipelines on a theoretical level. First consider the pipeline that involves dataframes which I visualized below. Notice that every intermediary stage in this pipeline takes a dataframe as input and outputs a dataframe, so it is difficult to tell the structure of the data without either checking it at runtime or remembering the expected structure of the input data and reading through the body - a task that becomes difficult as your project grows.</p>
<pre><code>List[Dict[str, float]]
    -(make_iris_dataframe)&gt; pd.DataFrame 
    -(calc_iris_area)&gt; pd.DataFrame
    -(filter_lower_sepal_quartile)&gt; pd.DataFrame
    -(av_area_by_species)&gt; pd.DataFrame
    -(plot_sepal_area)&gt; plotly.Plot
</code></pre>
<p>In contrast, the custom data type approach easily allows us to understand the structure that this data takes at each point in the pipeline. For instance, we know that at some point in our pipeline, the relevant data can be represented simply as a set of <code>IrisAreas</code> objects, and from the defintion we know what we expect to be the types of the data in those positions.</p>
<pre><code>List[Dict[str, float]]
    -(Irises.from_dicts)&gt; Irises (List[IrisEntry])
    -(.calc_areas)&gt; IrisAreas (List[IrisArea])
    -(.filter_lower_sepal_quartile)&gt; IrisAreas (List[IrisArea])
    -(.av_area_by_species)&gt; Dict[str, IrisArea]
    -(.plot.bar)&gt; plotly.Plot
</code></pre>
<ul>
<li>
<p><strong>More readible</strong>: the reader can identify the structure of the data at any point in the pipeline simply by looking at the data types (with type hints), regaurdless of whether they know the structure of the original data.</p>
</li>
<li>
<p><strong>Easier to maintain</strong>: the data scientist could replace or modify sections of the pipeline without needing to examine transformations that occur before or after, since the structure of the data will remain the same as long as the expected types are the same.</p>
</li>
<li>
<p><strong>Less error prone</strong>: smart static analyzers (including AI-assisted ones) can identify issues with accessing attributes and the structure of your data before you ever run it because defined data types provide gaurantees about which attributes your data should contain.</p>
</li>
</ul>
<div id="snippets">.</div>

<h2>Apprndix: Full Code Examples</h2>
<p>These are the full code snippets for convenience.</p>
<h5>Dataframe Approach</h5>
<pre><code>def make_iris_dataframe(iris_data: typing.List[typing.Dict[str, typing.Union[float, str]]]) -&gt; pd.DataFrame:
    df = pd.DataFrame.from_records(iris_data)
    return df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]

def calc_iris_area(iris_df: pd.DataFrame) -&gt; pd.DataFrame:
    '''Multiplies length and width of sepal and petals of each iris.'''
    return pd.DataFrame({
        'sepal_area': iris_df['sepal_length'] * iris_df['sepal_width'],
        'petal_area': iris_df['petal_length'] * iris_df['petal_width'],
        'species': iris_df['species'],
    })

def calc_iris_area_inplace(iris_df: pd.DataFrame) -&gt; pd.DataFrame:
    '''Multiplies length and width of sepal and petals of each iris.'''
    return pd.DataFrame({
        'sepal_area': iris_df['sepal_length'] * iris_df['sepal_width'],
        'petal_area': iris_df['petal_length'] * iris_df['petal_width'],
        'species': iris_df['species'],
    })

def filter_lower_sepal_quartile(area_df: pd.DataFrame) -&gt; pd.DataFrame:
    v = area_df['sepal_area'].quantile(0.25)
    return area_df.query(f'sepal_area &gt; {v}')

def av_area_by_species(area_df: pd.DataFrame) -&gt; pd.DataFrame:
    '''Average iris areas by species.'''
    return area_df.groupby('species').mean().reset_index(inplace=False, drop=False)

import plotly.express as px
def plot_sepal_area(areas_by_species: pd.DataFrame) -&gt; pd.DataFrame:
    '''Plot average sepal area by species.'''
    return px.bar(areas_by_species, x='species', y='sepal_area')

iris_df = make_iris_dataframe(iris_data)
area_df = calc_iris_area(iris_df)
filtered_area_df = filter_lower_sepal_quartile(area_df)
area_by_species = av_area_by_species(filtered_area_df)
plot_sepal_area(area_by_species)
</code></pre>
<h5>Custom Type Approach</h5>
<pre><code>import dataclasses

@dataclasses.dataclass
class IrisEntry:
    sepal_length: float
    sepal_width: float
    petal_length: float
    petal_width: float
    species: str

    @classmethod
    def from_dict(cls, entry: typing.Dict[str, float]):
        return cls(
            sepal_length = entry['sepal_length'],
            sepal_width = entry['sepal_width'],
            petal_length = entry['petal_length'],
            petal_width = entry['petal_width'],
            species = entry['species'],
        )

    def calc_area(self):
        return IrisArea.calc_from_iris(self)

class Irises(typing.List[IrisEntry]):

    @classmethod
    def from_dicts(cls, iris_data: typing.List[typing.Dict[str,float]]):
        return cls([IrisEntry.from_dict(ie) for ie in iris_data])

    def calc_areas(self):
        return IrisAreas.calc_from_irises(self)

@dataclasses.dataclass
class IrisArea:
    sepal_area: float
    petal_area: float
    species: str

    @classmethod
    def calc_from_iris(cls, iris: IrisEntry):
        return cls(
            sepal_area = iris.sepal_length * iris.sepal_width, 
            petal_area = iris.petal_length * iris.petal_width, 
            species = iris.species,
        )

class IrisAreas(typing.List[IrisArea]):
    @classmethod
    def calc_from_irises(cls, irises: Irises):
        return IrisAreas([IrisArea.calc_from_iris(ir) for ir in irises])

    def filter_lower_sepal_quartile(self):
        v = list(sorted([ia.sepal_area for ia in self]))[len(self)//4]
        return self.__class__([ia for ia in self if ia.sepal_area &gt; v])

    def av_area_by_species(self) -&gt; typing.Dict[str, IrisArea]:
        return {spec: areas.average() for spec, areas in self.group_by_species().items()}

    def average(self) -&gt; IrisArea:
        return IrisArea(
            sepal_area = sum([ia.sepal_area for ia in self])/len(self),
            petal_area = sum([ia.petal_area for ia in self])/len(self),
            species = self[0].species,
        )

    def group_by_species(self):
        species_areas: typing.Dict[str, IrisAreas] = dict()
        for a in self:
            species_areas.setdefault(a.species, self.__class__())
            species_areas[a.species].append(a)
        return species_areas

    def plot(self):
        return IrisAreaPlotter.from_area_averages(self.average_by_species())


@dataclasses.dataclass
class IrisAreaPlotter:
    iris_area_df: pd.DataFrame

    @classmethod
    def from_area_averages(cls, area_by_species: typing.Dict[str, IrisArea]):
        df = pd.DataFrame([dataclasses.asdict(a) for a in area_by_species.values()])
        return cls(df)

    def bar(self):
        return px.bar(self.iris_area_df, x='species', y='sepal_area')

irises = Irises.from_dicts(iris_data)
iris_areas = irises.calc_areas()
filtered_iris_areas = iris_areas.filter_lower_sepal_quartile()
averaged_iris_areas = filtered_iris_areas.av_area_by_species()
iris_plotter = IrisAreaPlotter.from_area_averages(averaged_iris_areas)
iris_plotter.bar()
</code></pre>
<h4>VVVVVVVVVVV ALL EXPERIMENTAL VVVVVVVVVVV</h4>
<p>They do not appear in your code except at the type of conversion or enforcement, or, even more sketchy. </p>
<p>but my concern is that the types of individual columns are never known by your interpreter until you actually run the code. While you may specify and enforce column  </p>
<p>or contained elements are not known by your interpreter</p>
<p>by the reader or static analyzer prior to runtime. You can specify and enforce the types of columns in dataframes, but your interpreter or analyzer never actually</p>
<p>they are not probject-specific - that is, they do not enforce structures that are relevant to the specific project for which the pipeline is being built. When you read a dataframe from a csv file, for example, you can specify the code </p>
<p>As such, simply knowing the type of an object does not give us insight into the representations that appear in the pipeline. </p>
<p>Dataframes maintain types for the columns they maintain, however, you cannot see the types unless you do some introspection into the </p>
<p>For a further elaboration on what I mean by adding more structure, see </p>
<p>. As an example, if you read a csv file as a dataframe, consider creating a class definition that represents a single row of that dataframe and include the code to parse that data within the same class, as well as any methods that operate on that class' data. Then encapsulate those objects into collections in which you can build additional methods for parsing, grouping, filtering, or transforming collections/lists/etc. By defining classes explicitly, your analyzer knows which attributes and methods are available on that object at any point in time. Avoid using lists of dictionaries or other datastructures without defined types, as they have the same pitfalls as dataframes.</p>
<h4>^^^^^^^^^^^^^^^^^</h4>
<p>Hiiiii^[Note that dataframes themselves are types and their columns have specific types within those objects, but the defining characteristic is that the interpreter or analyzer cannot infer those types without looking at the behavior of the functions or scripts used to produce it (which they often do not). They are types within the underlying package code, but they are not considered as types within the language itself. If you build your pipelines using functions that both accept and return dataframes, you do not know the structure of the new dataframe unless you look at the code used to transform it. In contrast, if you define custom types for the input and output data, you can know without looking at the]</p>
<h4>^^^^^^^^^^^^^^^^^</h4>
<h2>Data pipelines: separating the "what" from the "how"</h2>
<p>The topmost path in the figure shows the case where we do not keep track of the structure of the input or intermediate data in our code explicitly (imagine using a list of dictionaries or a dataframe read from a csv file), wheras in the bottom pipeline we represent them as objects A, B, and C explicitly in our code. The idea is that pipelines with explicit references to data structure in the code make it easier to understand what each transformation is doing - in theory, we (and the static analyzer in your IDE) could understand the entire pipeline without ever <em>running</em> our code.</p>
<h4>^^^^^^^^^^^^^^^^^^^</h4>
<h2>Debugging Pipelines</h2>
<p>Let us explore the case where you do not use custom data objects, and instead use dataframes or lists of dictionary/collections, or some other non-explicit data structures. As a hypothetical, say you are seeing a potential issue in your final data structure - a figure, let's say - and you want to investigate why you observe a given value. First, you hypothesize that the issue may have been with function/script 2, and so we first need to understand the structure of the intermediary data which it transformed. There are three approaches to understanding the intermediate data structure when we have not been explicit in our code: </p>
<ol>
<li>
<p>remember the structure of that data - generally a bad thing to rely on in software design because you may be looking at this years later or someone else may be looking at it;</p>
</li>
<li>
<p>run the first pipeline component and use some runtime introspection tool (breakpoints, print statements, debuggers, etc) to look at the data - possible but clunky and time-consuming; or </p>
</li>
<li>
<p>do some mental bookkeeping to trace the original input data (which may also require introspection) through the pipeline - also a time-consuming activity. </p>
</li>
</ol>
<p>None of these options look good - the best scenario is option 3, and even that is only viable if you know both the structure of the input data and are okay reading through the logic up until that point. Unfortunately, debugging or changing intermediary stages of your data pipeline will happen all the time - this can create some big problems as your project grows and your requirements change.</p>
<p>What is the problem with running the code in real time? In my experience, this simply takes a lot longer than keeping track of the code itself (either reading it or using a static analyzer) when it comes to large data pipelines. Each step or set of steps in your pipeline are expensive and probably time-consuming. To make it easier, you might optimize the pipeline by storing intermediary steps (RData or pickle files) so you can load them into separate notebooks more quickly, but this optimization is time-consuming and would need to be done every time you set out to work. In software engineering, it is generally far better to detect any problems without needing to actually run your code.</p>
<p>In the case where you represent the structure of your data as part of the code itself (i.e. use classes/structs to define intermediate structures), however, you (and your compiler/static analyzer) know the structure of the data at every stage of the pipeline because it is explicitly defined. From this alone you know not only that your data will appear in the specified formats (providing some gaurantees), but also that the role of that particular function/script is to convert data of type B to type C. In this case, the pipeline issue will be much easier to identify.</p>
<h3>Case Against Dataframes</h3>
<p>While dataframes are important data structures that a large suite of languages and packages have been built around, I have two primary concerns about using them as a central feature of your data pipelines: (1) all of the problems we observe above, and (2) they are often the wrong tools for the job (performance-wise) - even though they may be fine for many tasks involving small datasets.</p>
<p>The first point appears to be acceptable for many data scientists given that it is common to use Jupyter or R Markdown notebooks to write large portions of code. Except in initial development or in your toplevel scripts, I recommend using project file structures that are recommended for your language of choice - in Python, this means separating functions and classes (including the data containers) into modules, but there are equivalent recommended project structures for most langauges. Data science projects in particular tend to grow in scope or change in structure often, so modular project structures are especially important. The more complex your code becomes, the more important this is.</p>
<p>More concretely, lets refer to the iris example dataset we loaded. We access a column of that data using a subscript or as a property of the dataframe (although be careful with the latter):</p>
<pre><code>iris_df['species']
iris_df.species
</code></pre>
<p>Or, similarly in R:</p>
<pre><code>iris_df[:,'species']
iris_df$species
</code></pre>
<p>The problem with this is that you have no gaurantees that this property exists with this name in your input data. Even though the R and Python versions are both written as if the columns are object properties, they are not - they are simply syntactic sugar used to make it feel like they are - the reader, and your static analyzer, cannot gaurantee they exist except in runtime.</p>
<p>Sure, you could run a verification or transformation function that selects/orders columns and does some validation, but this code is implemented as part of the script loading the data, not in the definition of the data itself.</p>
<pre><code>iris_df = iris_df[['sepal_length', 'sepal_width', 'species']]
assert((iris_df['sepal_length']&gt;0).sum() == iris_df.shape[0])
</code></pre>
<p>In short, you have no gaurantees that a property will exist in runtime (assuming, as in the case of weakly typed languages, you follow the hints/gaurantees that you yourself provided).</p>
<p>I a also caution against using dataframes for performance reasons. Traditional data structures like <code>set</code>s, <code>dict</code>s (any kind of map), or <code>list</code>s (arrays) in Python all have implementations that are akin to different tasks. If you want to store an unordered set of unique elements, use a <code>set</code> object. If you want to perform lookups from a string or other hashable object to another object directly (i.e. in O(1) time), use a <code>dict</code>. If you want to store an ordered sequence of elements that can be indexed by their order in the sequence, use a <code>list</code>.</p>
<p>As an example of this issue, lets say that you want to perform a join on a two unindexed dataframes A with <code>n</code> rows and B with <code>m</code> rows that have a many-to-one relationship: that is there are zero or more rows of A that correspond to a single row of B. Because dataframe columns are implemented as sequential elements, the time it will take for this join to complete will be proportional to <code>n*m</code>: you would iterate through every element of A to find the associated row in B (note that this is worst case: <code>O(n*m)</code>). This is the wrong tool for the job because it could be done faster with a dictionary or hash map that allows you to look up the associated row in dataframe B instantly, so the time would be proportional to <code>n</code> (even if you have to create the dictionary first, you end up with <code>n + m ~ O(n)</code>).</p>
<p>Note that indexing dataframes is an operation that would allow for faster joins (typically through binary search, which is <code>O(n*log(m))</code>), indexing and multiindexing implementations are somewhat clunky to use and often involve explicit indexing between transformations. It is worth noting they exist, though I don't often see them used.</p>
<p>Dataframes can also be more memory intensive because joins and most other operations often create copies of data - even when it may be unnessecary. Be wary of this as your dataset gets larger.</p>
<p>As an alternative, I recommend creating an object to represent each row of your dataset, and parsing each row using a factory method - I will give some examples later.</p>
<h3>Nested Iterables are Also Bad</h3>
<p>It may also be tempting to use raw nested iterables like <code>set</code>s, <code>dict</code>s, or <code>list</code>s either because they follow directly from the structure of the input data (especially json data) or because they solve the second issue I have with dataframes - you can use the right tool for the job. My main concern with these structures is that they can get very complicated with high levels of nesting and requre missing data/error handling at every point of usage.</p>
<p>As another example, let us consider a json dataset parsed as a list of dictionaries with properties associated with irises. The type hint for the parsed structure would be <code>typing.List[typing.Dict[str, typing.Union[float, str]]]</code>.</p>
<pre><code>[
    {
        "sepal_length": 5.1, 
        "sepal_width": 3.5, 
        "species": "setosa"
    }, 
    {
        "sepal_length": 4.9, 
        "sepal_width": 3.0, 
        "species": "setosa"
    },
    ...
]
</code></pre>
<p>Sqy we want to get the average petal length for irises in our dataset, so we do this:</p>
<pre><code>import statistics
statistics.mean([iris['petal_length'] for iris in irises])
</code></pre>
<p>The problem here is the same as that of selecting columns in dataframes: we have no gaurantees that each iris will include a member called 'petal_length' until runtime. We can't know it exists unless we recall the data being passed in, and static analysis tools cannot help us.</p>
<p>The solution to these problems would again be to create a class representing a single Iris object and parsing them into a list of these objects. Even better, we could encapsulate the sequence of irises to add additional convenience. I will show some examples in the next sections.</p>
                    </div>
                </div>
            </div>
        </article>
        <!-- Footer-->
        <footer class="border-top">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <ul class="list-inline text-center">
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-facebook-f fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                        </ul>
                        <div class="small text-center text-muted fst-italic">Copyright &copy; Devin J. Cornell 2021<br/><hr/></div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="/js/blog.js"></script>
    </body>
</html>